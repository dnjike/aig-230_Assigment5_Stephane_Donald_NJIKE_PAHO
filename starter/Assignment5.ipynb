{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use this NLTK corpus:\n",
        "\n",
        "Option A (recommended): Gutenberg - 'austen-emma.txt' or 'carroll-alice.txt'\n",
        "\n",
        "Option B: Brown corpus (choose 1-2 categories, e.g., 'news' and 'romance')\n",
        "\n",
        "Choose ONE option and state your choice at the top of your notebook/report.\n",
        "You must complete all three parts below. Each part builds on the previous parts.\n"
      ],
      "metadata": {
        "id": "FgXBlEhjL-oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part A - Text Preprocessing (50%)**\n",
        "\n",
        "Goal: Prepare a clean token stream and justify your choices.\n",
        "A1. Load the corpus\n",
        "Load the raw text (or sentences) from your chosen NLTK corpus and print:\n",
        "Total number of characters (if raw text) or total number of sentences (if sentence-based)\n",
        "Total number of tokens BEFORE preprocessing\n"
      ],
      "metadata": {
        "id": "HtlAhxShMMcG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgINZu-tKhE0",
        "outputId": "5bdc494e-bfaf-4148-fae5-3b4118106468"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 144395\n",
            "Total tokens BEFORE preprocessing: 33535\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab') # Added to resolve LookupError\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import string\n",
        "\n",
        "# Load raw text\n",
        "raw_text = gutenberg.raw('carroll-alice.txt')\n",
        "\n",
        "print(\"Total characters:\", len(raw_text))\n",
        "\n",
        "# Token count BEFORE preprocessing\n",
        "tokens_before = word_tokenize(raw_text)\n",
        "print(\"Total tokens BEFORE preprocessing:\", len(tokens_before))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **A2. Preprocess**\n",
        "\n",
        "Create a preprocessing function that performs:\n",
        "\n",
        "Lowercasing\n",
        "Tokenization\n",
        "\n",
        "Removal of punctuation tokens\n",
        "\n",
        "Optional: stopword removal (if you choose to remove stopwords, explain why)\n",
        "\n",
        "Optional: stemming OR lemmatization (choose one if you use it, explain why)\n"
      ],
      "metadata": {
        "id": "TJoB03ZaNCG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # Remove punctuation and non-alphabetic tokens\n",
        "    tokens = [t for t in tokens if t.isalpha()]\n",
        "\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokens_after = preprocess(raw_text)\n",
        "\n",
        "print(\"Total tokens AFTER preprocessing:\", len(tokens_after))\n",
        "print(\"Vocabulary size:\", len(set(tokens_after)))\n",
        "\n",
        "# Top 20 frequent tokens\n",
        "freq = Counter(tokens_after)\n",
        "print(\"\\nTop 20 most frequent tokens:\")\n",
        "for word, count in freq.most_common(20):\n",
        "    print(word, count)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCyiuzw6NbBK",
        "outputId": "36b9d49a-5141-4a5e-f564-b63e230b93c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total tokens AFTER preprocessing: 25499\n",
            "Vocabulary size: 2293\n",
            "\n",
            "Top 20 most frequent tokens:\n",
            "the 1616\n",
            "a 887\n",
            "and 810\n",
            "to 720\n",
            "it 597\n",
            "she 545\n",
            "i 542\n",
            "of 499\n",
            "said 462\n",
            "alice 397\n",
            "wa 367\n",
            "in 359\n",
            "you 359\n",
            "that 284\n",
            "her 248\n",
            "at 209\n",
            "on 191\n",
            "had 185\n",
            "with 179\n",
            "all 178\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report these statistics AFTER preprocessing:\n",
        "\n",
        "Total number of tokens\n",
        "\n",
        "Vocabulary size (unique tokens)\n",
        "\n",
        "Top 20 most frequent tokens (with counts)\n",
        "\n",
        "# **A3. Reflection**\n",
        "\n",
        "Preprocessing really shapes how sparse data is and how well models perform. Lowercasing merges words like “Alice” and “alice,” so your vocabulary doesn’t explode. Dropping punctuation cuts out useless tokens that mess up vectors. Lemmatization simplifies forms like “running” to “run,” making stats more solid for frequency-based models.\n",
        "Keep stopwords for language models—they carry grammar and context. But in Bag-of-Words, they dominate and create issues. So, balance cleaning noise with keeping useful info, based on your task.\n"
      ],
      "metadata": {
        "id": "GL9zpcyqNjYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part B – Text Representation**"
      ],
      "metadata": {
        "id": "UEODy-dsRoWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Goal: Compare Bag-of-Words and TF-IDF representations and interpret the results.\n",
        "B1. Create documents\n",
        "\n",
        "Split your corpus into documents. Choose ONE of the following strategies and justify it:\n",
        "\n",
        "**Split into fixed-size chunks of 600 tokens.**\n",
        "The novel is shorter than Emma, Chunking ensures enough context per document, Avoids extremely small documents (like sentences).\n"
      ],
      "metadata": {
        "id": "5UlesxwURtIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_size = 600\n",
        "documents = []\n",
        "\n",
        "for i in range(0, len(tokens_after), chunk_size):\n",
        "    chunk = tokens_after[i:i+chunk_size]\n",
        "    documents.append(\" \".join(chunk))\n",
        "\n",
        "print(\"Number of documents:\", len(documents))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpEk78QnSPBn",
        "outputId": "4c6efa05-335c-44f3-8834-3a7f5cf79db4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 43\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**B2. Vectorization**"
      ],
      "metadata": {
        "id": "ixE_zIIhSWbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# Bag-of-Words\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "print(\"BoW shape:\", bow_matrix.shape)\n",
        "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQiQuRpJSaA2",
        "outputId": "8e338f46-cc92-4185-d6b6-6134b3714508"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BoW shape: (43, 2283)\n",
            "TF-IDF shape: (43, 2283)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top 15 TF-IDF Terms for 2 Documents"
      ],
      "metadata": {
        "id": "um_Y79AGSgHl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def top_tfidf_terms(doc_index, top_n=15):\n",
        "    row = tfidf_matrix[doc_index].toarray().flatten()\n",
        "    top_indices = row.argsort()[-top_n:][::-1]\n",
        "    return [(feature_names[i], row[i]) for i in top_indices]\n",
        "\n",
        "print(\"Top TF-IDF terms for Document 0:\")\n",
        "print(top_tfidf_terms(0))\n",
        "\n",
        "print(\"\\nTop TF-IDF terms for Document 3:\")\n",
        "print(top_tfidf_terms(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8zo5tRzShsT",
        "outputId": "d4c0de45-dcd2-4456-af73-52222d2a493b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top TF-IDF terms for Document 0:\n",
            "[('the', np.float64(0.3449720033229098)), ('to', np.float64(0.2973896580369912)), ('it', np.float64(0.2498073127510726)), ('she', np.float64(0.2498073127510726)), ('of', np.float64(0.202224967465154)), ('wa', np.float64(0.19032938114367434)), ('down', np.float64(0.18567486472003328)), ('and', np.float64(0.1784337948221947)), ('her', np.float64(0.1655631079451196)), ('picture', np.float64(0.12125986958676195)), ('very', np.float64(0.11997474266704955)), ('fell', np.float64(0.11329659945654286)), ('alice', np.float64(0.10952154108009704)), ('in', np.float64(0.10706027689331682)), ('jar', np.float64(0.09733069729752344))]\n",
            "\n",
            "Top TF-IDF terms for Document 3:\n",
            "[('to', np.float64(0.3239881019192489)), ('the', np.float64(0.29906594023315286)), ('she', np.float64(0.2741437785470568)), ('and', np.float64(0.22429945517486466)), ('it', np.float64(0.16199405095962446)), ('way', np.float64(0.15510220434605176)), ('cake', np.float64(0.1377788317574055)), ('wa', np.float64(0.1370718892735284)), ('foot', np.float64(0.1337166705110608)), ('herself', np.float64(0.12836728323972224)), ('alice', np.float64(0.12747555088188733)), ('of', np.float64(0.12461080843048036)), ('cry', np.float64(0.11868251357406016)), ('be', np.float64(0.10198044070550988)), ('on', np.float64(0.0996886467443843))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation Example: In Alice in Wonderland, TF-IDF often highlights:\n",
        "Character names: \"alice\", \"hatter\", \"queen\", \"rabbit\"; Scene-specific words: \"tea\", \"trial\", \"garden\". These terms appear strongly in particular sections but not across the entire book, making them distinctive."
      ],
      "metadata": {
        "id": "-UMq5NaGSl8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B3. Cosine Similarity"
      ],
      "metadata": {
        "id": "H-js_GRTS1Wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# Remove self similarity\n",
        "np.fill_diagonal(similarity_matrix, 0)\n",
        "\n",
        "max_sim = similarity_matrix.max()\n",
        "indices = np.where(similarity_matrix == max_sim)\n",
        "\n",
        "print(\"Most similar pair of documents:\", indices[0][0], \"and\", indices[1][0])\n",
        "print(\"Similarity score:\", max_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vx9jyXUSyLs",
        "outputId": "a20f5e82-8157-433a-ab67-6861a9b4b25c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar pair of documents: 37 and 38\n",
            "Similarity score: 0.7965779195927278\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highly similar chunks often correspond to continuous narrative scenes (e.g., Mad Hatter tea party).\n",
        "A surprising similarity may occur between distant chunks that both focus heavily on the Queen or courtroom scene."
      ],
      "metadata": {
        "id": "zm9BQQ9kS-WN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Part C – Word Embeddings (25%)**"
      ],
      "metadata": {
        "id": "x6hEeyW6TCDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "C1. Prepare Sentences"
      ],
      "metadata": {
        "id": "nv2Kjx3pTK1W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences_raw = sent_tokenize(raw_text)\n",
        "\n",
        "sentences = [preprocess(sentence) for sentence in sentences_raw]\n"
      ],
      "metadata": {
        "id": "Dx9OzxjaTMRx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "C2. Train Word2Vec"
      ],
      "metadata": {
        "id": "-WuVjGVzTUWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,  # Changed from 3 to 1 to include less frequent words\n",
        "    sg=1,      # Skip-gram\n",
        "    epochs=15\n",
        ")\n",
        "\n",
        "print(\"Word2Vec vocabulary size:\", len(model.wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbqT6SXVTanK",
        "outputId": "86ebb836-1bf0-4a04-e995-ba66482ad6fe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.1)\n",
            "Word2Vec vocabulary size: 2294\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C3. Most Similar Words"
      ],
      "metadata": {
        "id": "A_hXsP0IT2T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_words = [\"alice\", \"queen\", \"rabbit\", \"hatter\", \"king\"]\n",
        "\n",
        "for word in target_words:\n",
        "    print(f\"\\nTop 10 words similar to '{word}':\")\n",
        "    for sim_word, score in model.wv.most_similar(word, topn=10):\n",
        "        print(sim_word, score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b3eCLCQT7Nk",
        "outputId": "895a658e-d9ab-47ae-93c4-34e20f47662f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 10 words similar to 'alice':\n",
            "much 0.856599748134613\n",
            "remark 0.854781985282898\n",
            "curious 0.8514491319656372\n",
            "rather 0.8483914732933044\n",
            "certainly 0.8437233567237854\n",
            "poor 0.841570258140564\n",
            "very 0.8352744579315186\n",
            "herself 0.8340606093406677\n",
            "right 0.8322117924690247\n",
            "never 0.8315980434417725\n",
            "\n",
            "Top 10 words similar to 'queen':\n",
            "verse 0.9257924556732178\n",
            "turning 0.918398916721344\n",
            "king 0.9160270690917969\n",
            "executioner 0.9142700433731079\n",
            "jury 0.9139493107795715\n",
            "shouted 0.9110017418861389\n",
            "heart 0.9057885408401489\n",
            "shrill 0.9005929827690125\n",
            "pointing 0.8996469378471375\n",
            "teacup 0.8989439606666565\n",
            "\n",
            "Top 10 words similar to 'rabbit':\n",
            "white 0.9212437272071838\n",
            "kid 0.9016737937927246\n",
            "glove 0.8977483510971069\n",
            "court 0.8786617517471313\n",
            "low 0.8766154646873474\n",
            "hurried 0.8696526885032654\n",
            "fan 0.8694412112236023\n",
            "pair 0.8631691932678223\n",
            "blew 0.8622889518737793\n",
            "took 0.8583149909973145\n",
            "\n",
            "Top 10 words similar to 'hatter':\n",
            "king 0.9272031188011169\n",
            "angrily 0.9106067419052124\n",
            "march 0.8996372222900391\n",
            "duchess 0.898351788520813\n",
            "gryphon 0.8908795118331909\n",
            "interrupted 0.885306715965271\n",
            "hare 0.884377658367157\n",
            "added 0.8798806071281433\n",
            "course 0.8766054511070251\n",
            "jury 0.8702588081359863\n",
            "\n",
            "Top 10 words similar to 'king':\n",
            "hatter 0.9272030591964722\n",
            "queen 0.9160270094871521\n",
            "jury 0.8956128358840942\n",
            "turning 0.8838852047920227\n",
            "angrily 0.8831555843353271\n",
            "executioner 0.8568520545959473\n",
            "gryphon 0.8535382747650146\n",
            "interrupted 0.8533997535705566\n",
            "pointing 0.8516252636909485\n",
            "majesty 0.8472000956535339\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "C4. Analogies"
      ],
      "metadata": {
        "id": "y8ZdlPuqT_14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"king - man + woman ≈\")\n",
        "print(model.wv.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"]))\n",
        "\n",
        "print(\"\\nqueen - woman + man ≈\")\n",
        "print(model.wv.most_similar(positive=[\"queen\", \"man\"], negative=[\"woman\"]))\n",
        "\n",
        "print(\"\\nalice - girl + boy ≈\")\n",
        "print(model.wv.most_similar(positive=[\"alice\", \"boy\"], negative=[\"girl\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWU-LZsCUBws",
        "outputId": "a674e28b-7eda-47c9-c9d9-ac01d3e7be68"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "king - man + woman ≈\n",
            "[('executioner', 0.9122945666313171), ('turning', 0.9034780859947205), ('jury', 0.8967708349227905), ('pointing', 0.8936290144920349), ('dormouse', 0.8914698362350464), ('eagerly', 0.8903188705444336), ('tea', 0.8832041621208191), ('knave', 0.8823738694190979), ('queen', 0.8801239728927612), ('hatter', 0.8761951923370361)]\n",
            "\n",
            "queen - woman + man ≈\n",
            "[('king', 0.9164907932281494), ('jury', 0.8530906438827515), ('hatter', 0.8434901833534241), ('turning', 0.841728687286377), ('verse', 0.8373295664787292), ('angrily', 0.8330967426300049), ('interrupted', 0.8316513299942017), ('gryphon', 0.8281545639038086), ('heart', 0.8279138803482056), ('place', 0.8225963115692139)]\n",
            "\n",
            "alice - girl + boy ≈\n",
            "[('hastily', 0.8559770584106445), ('eagerly', 0.8492262363433838), ('turning', 0.8454760909080505), ('executioner', 0.8416664004325867), ('tea', 0.8394157290458679), ('indignantly', 0.8390999436378479), ('itself', 0.8331753015518188), ('decidedly', 0.8312958478927612), ('turned', 0.8256411552429199), ('twinkle', 0.8253844380378723)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation: The corpus is relatively small; Limited vocabulary diversity."
      ],
      "metadata": {
        "id": "qoM5H-bFUcfx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FyO-GEnYUgoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}